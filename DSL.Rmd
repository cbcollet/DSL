---
title: "DSL"
author: "Chu LI"
date: "2025-02-20"
output: html_document
---

```{r}
install.packages("readxl")  # Run this only once
library(readxl)

divorce_data <- read_excel("divorce.xlsx")  # adjust the filename if needed
head(divorce_data)
str(divorce_data)

```
```{r}
install.packages("dplyr")  # Run this only once
```
```{r}
summary(divorce_data)

```

```{r}
library(dplyr)
```
```{r}
str(divorce_data)
names(divorce_data)
```

```{r}
names(divorce_data)[length(names(divorce_data))] 
# Get the last column's name
```

```{r}
#To confirm that "Divorce_Y_N" is correct
table(divorce_data$Divorce_Y_N)
```

```{r}
#check for mising values
sum(is.na(divorce_data))
colSums(is.na(divorce_data))

```

```{r}
# In this section we sorted the question we thought were a topic of communication behavior together to see the average score of these questions against the divorce Y N (C.Collet)
library(dplyr)

divorce_data_groups <- divorce_data %>%
  mutate(
    communication_behavior = rowMeans(select(., Sorry_end, begin_correct, Contact, Aggro_argue, Always_never, 
                                              negative_personality, offensive_expressions, insult, humiliate, not_calm, 
                                              hate_subjects, sudden_discussion, `idk_what's_going_on`, calm_breaks, 
                                              argue_then_leave, silent_for_calm, good_to_leave_home, 
                                              silence_instead_of_discussion, silence_for_harm, silence_fear_anger, 
                                              `I'm_right`, accusations, `I'm_not_guilty`, `I'm_not_wrong`, 
                                              no_hesitancy_inadequate, `you're_inadequate`, incompetence), na.rm = TRUE)
  )
head(divorce_data_groups[, c("communication_behavior", "Divorce_Y_N")])
communication_data <- divorce_data_groups%>% select(communication_behavior, Divorce_Y_N)
```

```{r}
# Here we did the same thing, but for questions that we consider value_alignment. (C.Collet)

divorce_data_va <- divorce_data %>%
  mutate(
    value_alignment = rowMeans(select(., enjoy_holiday, enjoy_travel, common_goals, harmony, freeom_value, 
                                      entertain, people_goals, dreams, love, happy, marriage, roles, trust, 
                                      likes, care_sick, fav_food, stresses, inner_world, anxieties, 
                                      current_stress, hopes_wishes, know_well, friends_social),
                               na.rm = TRUE)
  )
# New dataset with value_alignment and Divorce_Y_N
value_alignment_data <- divorce_data_va %>% select(value_alignment, Divorce_Y_N)


```

```{r}

# And again, but now for questions we consider to be relevant for Knowledge of spouse. We now have 3 groups: Communication behavior, value alignment and knowledge of spouse. (C.Collet)

divorce_data_kos <- divorce_data %>%
  mutate(
    knowledge_of_spouse = rowMeans(select(., likes, care_sick, fav_food, stresses, inner_world, anxieties, 
                                         current_stress, hopes_wishes, know_well, friends_social), na.rm = TRUE)
  )

# Create a separate dataset for knowledge_of_spouse and Divorce_Y_N
knowledge_of_spouse_data <- divorce_data_kos %>% select(knowledge_of_spouse, Divorce_Y_N)


```

```{r}
install.packages("caret")
install.packages("randomForest")
```
```{r}
# I tried to get the optimal amount of variables and then select the most important ones that change the chance of prediction the most.
library(caret)
library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)  # Using Random Forest for feature ranking
set.seed(123)  # For reproducibility
rfe_result <- rfe(x=divorce_data[, -55], y=divorce_data$Divorce_Y_N, 
                  sizes=c(5, 10, 15, 20),  # Different feature set sizes to test
                  rfeControl=control)
print(rfe_result)  # Shows selected features and performance
plot(rfe_result)   # Visualizes accuracy vs. number of features
selected_features <- predictors(rfe_result)
data_selected <- divorce_data[, c(selected_features, "Divorce_Y_N")]


predictors(rfe_result)
optimalnumberofpredictors <- predictors(rfe_result)
print(optimalnumberofpredictors)

```

```{r}
# This piece of code uses a logistic regression model with the 10 variables that where the most important. The output gives us an indication on how good the model is in predicting divorce. 

# Convert Divorce_Y_N to a factor (0 = No, 1 = Yes)
divorce_data$Divorce_Y_N <- as.factor(divorce_data$Divorce_Y_N)

# Check if conversion was successful
str(divorce_data$Divorce_Y_N)
table(divorce_data$Divorce_Y_N)  # Check distribution

set.seed(123) # Ensure reproducibility

# Create training and testing indices
install.packages("caret")
library(caret)
trainIndex <- createDataPartition(divorce_data$Divorce_Y_N, p = 0.8, list = FALSE)

# Split the dataset
trainData <- divorce_data[trainIndex, ]
testData  <- divorce_data[-trainIndex, ]

# Verify the split
table(trainData$Divorce_Y_N)  
table(testData$Divorce_Y_N)  

# Define the formula using the 10 selected features
formula <- Divorce_Y_N ~ `idk_what's_going_on` + marriage + anxieties + roles + harmony +
                         happy + enjoy_travel + trust + hopes_wishes + sudden_discussion

# Train logistic regression model
logit_model <- glm(formula, data = trainData, family = binomial)

# Print model summary
summary(logit_model)

# Predict probabilities on test data
pred_probs <- predict(logit_model, newdata = testData, type = "response")

# Convert probabilities to binary predictions (Yes/No)
pred_classes <- ifelse(pred_probs > 0.5, "Yes", "No")

# Convert to factors to match test data
pred_classes <- as.factor(pred_classes)

# View first few predictions
head(pred_classes)

# Check the levels of both factors
levels(pred_classes)
levels(testData$Divorce_Y_N)

# Align levels of both predicted and actual values
levels(pred_classes) <- c("No", "Yes")  # Adjust these as per your actual labels
levels(testData$Divorce_Y_N) <- c("No", "Yes")  # Adjust these as per your actual labels

# Now run confusion matrix again
conf_matrix <- confusionMatrix(pred_classes, testData$Divorce_Y_N)

# Print results
print(conf_matrix)


```


```{r}
#Handle illegal characters in the variable name before proceeding with the test(C.L)

# Convert feature names to valid R variable names
top_10_features_clean <- make.names(top_10_features)

# Remove single quotes and other problematic characters
top_10_features_clean <- gsub("['\"()&]", "", top_10_features_clean)  # Remove quotes, brackets, & symbol
top_10_features_clean <- gsub("[^A-Za-z0-9_]", "_", top_10_features_clean)  # Replace other special characters with underscores

# Create formula with cleaned feature names
formula_top10 <- as.formula(paste("Divorce_Y_N ~", paste(top_10_features_clean, collapse = " + ")))

# Print to check if formula is correctly formatted
print(formula_top10)

```

```{r}
print(top_10_features_clean)
```


```{r}
#First, we split the data into two parts: one part was used to train the model and the other part was used to test the model's effectiveness. Then we made three different predictions, testing the effect of using all factors, using the 10 most important factors, and grouping the factors together.(C.L)

# Load necessary libraries
library(caret)
library(randomForest)
library(dplyr)

# Ensure the Divorce_Y_N variable is a factor (0 = No, 1 = Yes)
# This ensures the target variable is treated as a categorical factor, crucial for classification tasks.
divorce_data$Divorce_Y_N <- as.factor(divorce_data$Divorce_Y_N)

# Set seed for reproducibility
set.seed(123)

# Split the dataset into training and testing sets (80% training, 20% testing)
trainIndex <- createDataPartition(divorce_data$Divorce_Y_N, p = 0.8, list = FALSE)
trainData <- divorce_data[trainIndex, ]
testData  <- divorce_data[-trainIndex, ]

# -----------------------------------------------
# 1. Model using ALL available variables
# -----------------------------------------------
# Define the formula using all predictors (clean feature names)
formula_all <- as.formula(paste("Divorce_Y_N ~", paste(names(divorce_data)[-length(names(divorce_data))], collapse = " + ")))

# Train logistic regression model with all features
logit_model_all <- glm(formula_all, data = trainData, family = binomial)

# Predict probabilities on test data
pred_probs_all <- predict(logit_model_all, newdata = testData, type = "response")

# Convert probabilities to binary predictions
pred_classes_all <- ifelse(pred_probs_all > 0.5, "Yes", "No")
pred_classes_all <- as.factor(pred_classes_all)

# Ensure both are factors and have the same levels
pred_classes_all <- factor(pred_classes_all, levels = levels(testData$Divorce_Y_N))
testData$Divorce_Y_N <- factor(testData$Divorce_Y_N)

# Now compute confusion matrix
conf_matrix_all <- confusionMatrix(pred_classes_all, testData$Divorce_Y_N)

# Evaluate model performance
print(conf_matrix_all)

# -----------------------------------------------
# 2. Model using ONLY the 10 most important features
# -----------------------------------------------
# Select the top 10 features identified earlier
top_10_features <- c("idk_what's_going_on", "marriage", "anxieties", "roles", "harmony",
                     "happy", "enjoy_travel", "trust", "hopes_wishes", "sudden_discussion")

# Ensure feature names are valid for formula
top_10_features_clean <- make.names(top_10_features)

# Create formula with cleaned feature names
formula_top10 <- as.formula(paste("Divorce_Y_N ~", paste(top_10_features_clean, collapse = " + ")))

# Train logistic regression model with top 10 features
logit_model_top10 <- glm(formula_top10, data = trainData, family = binomial)

# Predict probabilities on test data
pred_probs_top10 <- predict(logit_model_top10, newdata = testData, type = "response")

# Convert probabilities to binary predictions
pred_classes_top10 <- ifelse(pred_probs_top10 > 0.5, "Yes", "No")
pred_classes_top10 <- as.factor(pred_classes_top10)

# Ensure both prediction and actual values have the same levels
pred_classes_top10 <- factor(pred_classes_top10, levels = levels(testData$Divorce_Y_N))

# Now compute confusion matrix
conf_matrix_top10 <- confusionMatrix(pred_classes_top10, testData$Divorce_Y_N)

# Evaluate model performance
conf_matrix_top10 <- confusionMatrix(pred_classes_top10, testData$Divorce_Y_N)

# Print results
print(conf_matrix_top10)

# -----------------------------------------------
# 3. Categorizing variables into different groups
# -----------------------------------------------
# Define grouped feature sets
group_communication <- c("Sorry_end", "begin_correct", "Contact", "Aggro_argue", "Always_never",
                         "negative_personality", "offensive_expressions", "insult", "humiliate", "not_calm",
                         "hate_subjects", "sudden_discussion", "idk_what's_going_on", "calm_breaks",
                         "argue_then_leave", "silent_for_calm", "good_to_leave_home",
                         "silence_instead_of_discussion", "silence_for_harm", "silence_fear_anger",
                         "I'm_right", "accusations", "I'm_not_guilty", "I'm_not_wrong",
                         "no_hesitancy_inadequate", "you're_inadequate", "incompetence")

group_value_alignment <- c("enjoy_holiday", "enjoy_travel", "common_goals", "harmony", "freeom_value",
                           "entertain", "people_goals", "dreams", "love", "happy", "marriage", "roles", "trust",
                           "likes", "care_sick", "fav_food", "stresses", "inner_world", "anxieties",
                           "current_stress", "hopes_wishes", "know_well", "friends_social")

group_knowledge_spouse <- c("likes", "care_sick", "fav_food", "stresses", "inner_world", "anxieties",
                            "current_stress", "hopes_wishes", "know_well", "friends_social")

# Ensure feature names are valid for formula in grouped features
group_communication <- make.names(group_communication)
group_value_alignment <- make.names(group_value_alignment)
group_knowledge_spouse <- make.names(group_knowledge_spouse)

# Create logistic regression models for each group

# Function to train model and print results
train_and_evaluate <- function(feature_set, group_name) {
  formula_group <- as.formula(paste("Divorce_Y_N ~", paste(feature_set, collapse = " + ")))
  
  model <- glm(formula_group, data = trainData, family = binomial)
  
  pred_probs <- predict(model, newdata = testData, type = "response")
  pred_classes <- ifelse(pred_probs > 0.5, "Yes", "No")
  pred_classes <- as.factor(pred_classes)
  
  conf_matrix <- confusionMatrix(pred_classes, testData$Divorce_Y_N)
  
  print(paste("Results for", group_name, "group:"))
  print(conf_matrix)
}

# Train and evaluate function with adjusted factor levels
train_and_evaluate <- function(feature_set, group_name) {
  formula_group <- as.formula(paste("Divorce_Y_N ~", paste(feature_set, collapse = " + ")))
  
  model <- glm(formula_group, data = trainData, family = binomial)
  
  pred_probs <- predict(model, newdata = testData, type = "response")
  pred_classes <- ifelse(pred_probs > 0.5, "Yes", "No")
  
  # Ensure both prediction and actual values have the same levels
  pred_classes <- factor(pred_classes, levels = levels(testData$Divorce_Y_N))
  
  # Compute confusion matrix
  conf_matrix <- confusionMatrix(pred_classes, testData$Divorce_Y_N)
  
  print(paste("Results for", group_name, "group:"))
  print(conf_matrix)
}


```
```{r}
# We may be experiencing data imbalance problems, variable selection and characterization problems, extreme predictions of the model......
```

