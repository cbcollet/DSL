---
title: "DSL FINAL CODE"
author: "Casper Collet and Chu Li"
date: "2025-03-29"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
---

```{r}
#Written by: Casper Collet
library(readxl)
divorce_data <- as.data.frame(read_excel("divorce.xlsx"))
head(divorce_data)
str(divorce_data)

#Written by: Casper Collet

#A couple of variables had backticks in them and needed to be changed to ensure we can work with the dataset.

colnames(divorce_data)[colnames(divorce_data) == "2_strangers"] <- "Two_Strangers"
colnames(divorce_data)[colnames(divorce_data) == "idk_what's_going_on"] <- "idk_what_is_going_on"
colnames(divorce_data)[colnames(divorce_data) == "I'm_right"] <- "I_Am_Right"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_wrong"] <- "I_am_not_wrong"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_guilty"] <- "I_am_not_guilty"
colnames(divorce_data)[colnames(divorce_data) ==  "you're_inadequate"] <- "you_are_inadequate"

#Our first task is: Split around 20% of the dataset of so we can keep some unseen data to test our models on after we tried them on the 80%. To keep it random, we will remove the last.

set.seed(123) 
sample_index <- sample(1:nrow(divorce_data), size = round(0.2 * nrow(divorce_data)))

Unseendata <- divorce_data[sample_index, ]
seendata <- divorce_data[-sample_index, ]

# (C.L) Set mutually exclusive row names
rownames(Unseendata) <- paste0("unseen_", 1:nrow(Unseendata))
rownames(seendata) <- paste0("seen_", 1:nrow(seendata))

# Verify no overlap
print(head(rownames(seendata)))
print(head(rownames(Unseendata)))
print(intersect(rownames(seendata), rownames(Unseendata)))

table(Unseendata$Divorce_Y_N, useNA = "always") 


```

```{r}
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.

library(randomForest)

#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.

seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)

rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
```

```{r}
#Written by Casper Collet
#Here we test the accuracy of the model on the unseendata, this came down to 100%, because it had perfect accuracy.
library(caret)
Unseendata$Divorce_Y_N <- factor(Unseendata$Divorce_Y_N, levels = levels(seendata$Divorce_Y_N))

unseen_predictions <- predict(rf_model, Unseendata)

# Evaluate the accuracy on Unseendata
conf_matrix_unseen <- confusionMatrix(unseen_predictions, Unseendata$Divorce_Y_N)
print(conf_matrix_unseen)


```

```{r}
#Written by: Casper Collet
#We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model) 
varImpPlot(rf_model) 
Randomforest_most_important_questions <- importance(rf_model) #To compare later on

```

```{r}
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights. 

bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)

print(bagging_model)

#We immediately see that in the bagging method, has one less falsely classified object in the confusion matrix (4 became 3). Next on we test the bagging method on the unseen data. This show us an accuracy of 77.14% which is less than the 100% from random forest.

unseen_predictions <- predict(bagging_model, Unseendata)

# Evaluate the accuracy on Unseendata
conf_matrix_unseenbag <- confusionMatrix(unseen_predictions, Unseendata$Divorce_Y_N)
print(conf_matrix_unseenbag)

#Here we show the most important variables from the bagging model.

importance(bagging_model)
varImpPlot(bagging_model)
Bagging_most_important_questions <- importance(bagging_model)
```

```{r}
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.(C.L)

# Load required packages
library(glmnet)

# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric response

# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
lasso_best_lambda <- lasso_model$lambda.min  # Get the best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)  # Extract coefficients
# Print results
print("Lasso Model Coefficients:")
print(lasso_coefficients)
```

```{r}
#Last but not least, the Ridge model.(C.L)
library(glmnet)
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients

print("Ridge Model Coefficients:")
print(ridge_coefficients)
```


```{r}
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance 
# (C.L)

# Load necessary libraries
library(glmnet)
library(pROC)
library(caret)

# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)

# Prepare data matrices for Lasso and Ridge (glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorce_Y_N))  # Convert factor to numeric

# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
lasso_pred <- predict(lasso_model, newx = x_test, s = "lambda.min", type = "response")
lasso_class <- ifelse(lasso_pred > 0.5, 1, 0)  # Convert probabilities to binary labels

# Train Ridge Model (L2 regularization)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
ridge_pred <- predict(ridge_model, newx = x_test, s = "lambda.min", type = "response")
ridge_class <- ifelse(ridge_pred > 0.5, 1, 0)  # Convert probabilities to binary labels

# Compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
  accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
  auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
  mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
  return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}

# Evaluate Lasso
lasso_results <- evaluate_model(y_test, lasso_class, lasso_pred)
print("Lasso Model Results:")
print(lasso_results)

# Evaluate Ridge
ridge_results <- evaluate_model(y_test, ridge_class, ridge_pred)
print("Ridge Model Results:")
print(ridge_results)

# Plot AUC-ROC Curves
roc_lasso <- roc(y_test, lasso_pred)
roc_ridge <- roc(y_test, ridge_pred)

plot(roc_lasso, col = "blue", main = "AUC-ROC Curves for Lasso and Ridge")
plot(roc_ridge, col = "red", add = TRUE)
legend("bottomright", legend = c("Lasso", "Ridge"), col = c("blue", "red"), lty = 1)
```

```{r}
# (C.L)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)

# Convert Divorce label to factor for classification
colnames(Unseendata)[colnames(Unseendata) == "Divorced"] <- "Divorce_Y_N"
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)

# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels

# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)

# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
  accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
  auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
  mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
  return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}

# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_class, rf_pred)
print("Random Forest Model Results:")
print(rf_results)

# Evaluate Bagging
bagging_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_class, bagging_pred)
print("Bagging Model Results:")
print(bagging_results)

# Plot AUC-ROC Curves
roc_rf <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_pred)
roc_bagging <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_pred)

plot(roc_rf, col = "blue", main = "AUC-ROC Curves for Random Forest and Bagging")
plot(roc_bagging, col = "red", add = TRUE)
legend("bottomright", legend = c("Random Forest", "Bagging"), col = c("blue", "red"), lty = 1)

```
## From now on are all the code to Check why all 4 of our predictions are overfit (accuracy=1, AUC=1)


```{r}
table(Unseendata$Divorce_Y_N)
```

```{r}
cor_matrix <- cor(divorce_data[, 1:54], as.numeric(divorce_data$Divorce_Y_N))
print(cor_matrix)
```
## The result shows that the correlation between the predictors and the response variable is very high. This might be the reason why the models are overfitting.



```{r}
intersect(rownames(seendata), rownames(Unseendata))
```

```{r}
nrow(seendata)  # should be 136
nrow(Unseendata)  # should be 34
nrow(seendata) + nrow(Unseendata)  # should be 170
```
## The results show that the number of rows in the training and testing datasets are correct.



```{r}
# Verify no overlap
if (length(intersect(rownames(seendata), rownames(Unseendata))) == 0) {
  print("Split successfully, no overlap")
} else {
  print("Segmentation failed, overlap exists")
}
```
## The results show that the split was failed, and there is an overlap between the training and testing datasets.







