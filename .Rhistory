# Convert Divorce_Y_N to factor
seendata <- seendata %>%
mutate(Divorce_Y_N = as_factor(Divorce_Y_N))
Unseendata <- Unseendata %>%
mutate(Divorce_Y_N = as_factor(Divorce_Y_N))
# Verify row counts
cat("Rows in seendata:", nrow(seendata), "\n")      # Should be 136
cat("Rows in Unseendata:", nrow(Unseendata), "\n")  # Should be 34
cat("Total rows:", nrow(seendata) + nrow(Unseendata), "\n")  # Should be 170
head(divorce_data)
str(divorce_data)
table(Unseendata$Divorce_Y_N, useNA = "always")
# Written by: Chu Li
# Summarize means/SDs
overall_summary <- divorce_data %>%
select(1:54) %>%
summarise(across(.cols = everything(),
.fns = list(mean = ~mean(., na.rm = TRUE),
sd = ~sd(., na.rm = TRUE))))
# Replace underscores in column names to avoid splitting issues
names(overall_summary) <- gsub("_mean", ".mean", names(overall_summary))
names(overall_summary) <- gsub("_sd", ".sd", names(overall_summary))
# Pivot with dot separator
overall_summary <- overall_summary %>%
pivot_longer(everything(), names_to = c("Variable", ".value"), names_sep = "\\.") %>%
summarise(mean_range = paste(round(range(mean, na.rm = TRUE), 3), collapse = "–"),
sd_range = paste(round(range(sd, na.rm = TRUE), 3), collapse = "–"))
print(overall_summary)
# Written by: C.L.
# Outcome summary
outcome_summary <- table(divorce_data$Divorce_Y_N)
prop.table(outcome_summary)
# Correlation with outcome
cor_matrix <- cor(divorce_data[, 1:54], as.numeric(divorce_data$Divorce_Y_N))
# Sort to find top correlations
sorted_cor <- sort(cor_matrix[,1], decreasing = TRUE)
print(head(sorted_cor, 5))  # Top 5: idk_what_is_going_on, happy, roles, marriage, trust
# Predictor summaries (mean, SD) for top 5 questions
top_questions <- names(head(sorted_cor, 5))
divorce_data %>%
summarise(across(all_of(top_questions), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))))
# Group differences for top 5 questions
group_means <- divorce_data %>%
group_by(Divorce_Y_N) %>%
summarise(across(all_of(top_questions), mean, na.rm = TRUE))
print(group_means)
# Correlation among predictors (check multicollinearity)
cor_predictors <- cor(divorce_data[, top_questions])
print(cor_predictors)
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
#Written by Casper Collet
#Here we test the accuracy of the model on the unseendata, this came down to 100%, because it had perfect accuracy.
library(caret)
Unseendata$Divorce_Y_N <- factor(Unseendata$Divorce_Y_N, levels = levels(seendata$Divorce_Y_N))
unseen_predictions <- predict(rf_model, Unseendata)
# Evaluate the accuracy on Unseendata
conf_matrix_unseen <- confusionMatrix(unseen_predictions, Unseendata$Divorce_Y_N)
print(conf_matrix_unseen)
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
print(bagging_model)
#We immediately see that in the bagging method, has one less falsely classified object in the confusion matrix (4 became 3). Next on we test the bagging method on the unseen data. This show us an accuracy of 77.14% which is less than the 100% from random forest.
unseen_predictions <- predict(bagging_model, Unseendata)
# Evaluate the accuracy on Unseendata
conf_matrix_unseenbag <- confusionMatrix(unseen_predictions, Unseendata$Divorce_Y_N)
print(conf_matrix_unseenbag)
#Here we show the most important variables from the bagging model.
importance(bagging_model)
varImpPlot(bagging_model)
Bagging_most_important_questions <- importance(bagging_model)
# written by: Casper Collet
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.
# Load required packages
library(glmnet)
# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric response
# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
lasso_best_lambda <- lasso_model$lambda.min  # Get the best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)  # Extract coefficients
# Print results
print("Lasso Model Coefficients:")
print(lasso_coefficients)
#Last but not least, the Ridge model.(C.L)
library(glmnet)
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients
print("Ridge Model Coefficients:")
print(ridge_coefficients)
# written by: Chu Li
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)
# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_class, rf_pred)
print("Random Forest Model Results:")
print(rf_results)
# Evaluate Bagging
bagging_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_class, bagging_pred)
print("Bagging Model Results:")
print(bagging_results)
# Plot AUC-ROC Curves
roc_rf <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_pred)
roc_bagging <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_pred)
plot(roc_rf, col = "blue", main = "AUC-ROC Curves for Random Forest and Bagging")
plot(roc_bagging, col = "red", add = TRUE)
legend("bottomright", legend = c("Random Forest", "Bagging"), col = c("blue", "red"), lty = 1)
# From now on are all the code to Check why all 4 of our predictions are overfit (accuracy=1, AUC=1)
cor_matrix <- cor(divorce_data[, 1:54], as.numeric(divorce_data$Divorce_Y_N))
print(cor_matrix)
# Verify no overlap
if (length(intersect(rownames(seendata), rownames(Unseendata))) == 0) {
print("Split successfully, no overlap")
} else {
print("Segmentation failed, overlap exists")
}
# written by: Chu Li
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)
# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_class, rf_pred)
print("Random Forest Model Results:")
print(rf_results)
# Evaluate Bagging
bagging_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_class, bagging_pred)
print("Bagging Model Results:")
print(bagging_results)
# Plot AUC-ROC Curves
roc_rf <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_pred)
roc_bagging <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_pred)
plot(roc_rf, col = "blue", main = "AUC-ROC Curves for Random Forest and Bagging")
plot(roc_bagging, col = "red", add = TRUE)
legend("bottomright", legend = c("Random Forest", "Bagging"), col = c("blue", "red"), lty = 1)
#Written by: Casper Collet & Chu Li
library(tidyverse)
library(readxl)
# Load dataset
data_file <- "divorce.xlsx"
if (!file.exists(data_file)) {
stop("Error: Data file 'divorce.xlsx' not found.")
}
divorce_data <- read_xlsx("divorce.xlsx")
# Check total rows
cat("Total rows in divorce_data:", nrow(divorce_data), "\n")
# Check for missing values
if (any(is.na(divorce_data))) {
warning("Missing values detected. Removing rows with NAs.")
divorce_data <- divorce_data %>%
drop_na()
cat("Rows after removing NAs:", nrow(divorce_data), "\n")
} else {
cat("No missing values detected.\n")
}
# Check for duplicates
duplicates <- divorce_data %>%
duplicated() %>%
sum()
cat("Number of duplicate rows:", duplicates, "\n")
# Add row index for splitting
divorce_data <- divorce_data %>%
mutate(.row = row_number())
# Split into 80% training and 20% testing sets
set.seed(123)
Unseendata <- divorce_data %>%
slice_sample(prop = 0.2)  # 34 rows
seendata <- divorce_data %>%
slice(-Unseendata$.row)   # Remaining 136 rows
# Remove temporary row index
Unseendata <- Unseendata %>% select(-.row)
seendata <- seendata %>% select(-.row)
# Rename columns
divorce_data <- divorce_data %>%
rename(
Two_Strangers = `2_strangers`,
idk_what_is_going_on = `idk_what's_going_on`,
I_Am_Right = `I'm_right`,
I_am_not_wrong = `I'm_not_wrong`,
I_am_not_guilty = `I'm_not_guilty`,
you_are_inadequate = `you're_inadequate`
)
Unseendata <- Unseendata %>%
rename(
Two_Strangers = `2_strangers`,
idk_what_is_going_on = `idk_what's_going_on`,
I_Am_Right = `I'm_right`,
I_am_not_wrong = `I'm_not_wrong`,
I_am_not_guilty = `I'm_not_guilty`,
you_are_inadequate = `you're_inadequate`
)
seendata <- seendata %>%
rename(
Two_Strangers = `2_strangers`,
idk_what_is_going_on = `idk_what's_going_on`,
I_Am_Right = `I'm_right`,
I_am_not_wrong = `I'm_not_wrong`,
I_am_not_guilty = `I'm_not_guilty`,
you_are_inadequate = `you're_inadequate`
)
# Convert Divorce_Y_N to factor
seendata <- seendata %>%
mutate(Divorce_Y_N = as_factor(Divorce_Y_N))
Unseendata <- Unseendata %>%
mutate(Divorce_Y_N = as_factor(Divorce_Y_N))
# Verify row counts
cat("Rows in seendata:", nrow(seendata), "\n")      # Should be 136
cat("Rows in Unseendata:", nrow(Unseendata), "\n")  # Should be 34
cat("Total rows:", nrow(seendata) + nrow(Unseendata), "\n")  # Should be 170
head(divorce_data)
str(divorce_data)
table(Unseendata$Divorce_Y_N, useNA = "always")
# Written by: Chu Li
# Summarize means/SDs
overall_summary <- divorce_data %>%
select(1:54) %>%
summarise(across(.cols = everything(),
.fns = list(mean = ~mean(., na.rm = TRUE),
sd = ~sd(., na.rm = TRUE))))
# Replace underscores in column names to avoid splitting issues
names(overall_summary) <- gsub("_mean", ".mean", names(overall_summary))
names(overall_summary) <- gsub("_sd", ".sd", names(overall_summary))
# Pivot with dot separator
overall_summary <- overall_summary %>%
pivot_longer(everything(), names_to = c("Variable", ".value"), names_sep = "\\.") %>%
summarise(mean_range = paste(round(range(mean, na.rm = TRUE), 3), collapse = "–"),
sd_range = paste(round(range(sd, na.rm = TRUE), 3), collapse = "–"))
print(overall_summary)
# Written by: Chu Li
# Outcome summary
outcome_summary <- table(divorce_data$Divorce_Y_N)
prop.table(outcome_summary)
# Correlation with outcome
cor_matrix <- cor(divorce_data[, 1:54], as.numeric(divorce_data$Divorce_Y_N))
# Sort to find top correlations
sorted_cor <- sort(cor_matrix[,1], decreasing = TRUE)
print(head(sorted_cor, 5))  # Top 5: idk_what_is_going_on, happy, roles, marriage, trust
# Predictor summaries (mean, SD) for top 5 questions
top_questions <- names(head(sorted_cor, 5))
divorce_data %>%
summarise(across(all_of(top_questions), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))))
# Group differences for top 5 questions
group_means <- divorce_data %>%
group_by(Divorce_Y_N) %>%
summarise(across(all_of(top_questions), mean, na.rm = TRUE))
print(group_means)
# Correlation among predictors (check multicollinearity)
cor_predictors <- cor(divorce_data[, top_questions])
print(cor_predictors)
# Written by: Chu Li
# Load ggplot2 for visualization
library(ggplot2)
# Data for top 3
top_vars <- c("idk_what_is_going_on", "happy", "trust")
plot_data <- divorce_data %>%
select(Divorce_Y_N, all_of(top_vars)) %>%
mutate(Divorce_Y_N = factor(Divorce_Y_N, levels = c(0, 1), labels = c("Not Divorced", "Divorced"))) %>%
pivot_longer(cols = all_of(top_vars), names_to = "Variable", values_to = "Score")
# Boxplot
ggplot(plot_data, aes(x = Divorce_Y_N, y = Score)) +
geom_boxplot() +
facet_wrap(~ Variable) +
labs(x = "Divorce Status", y = "Score")
# Data for lowest 5
low_vars <- c("No_home_time", "silence_for_harm", "Two_Strangers", "silent_for_calm", "silence_instead_of_discussion")
plot_data <- divorce_data %>%
select(Divorce_Y_N, all_of(low_vars)) %>%
mutate(Divorce_Y_N = factor(Divorce_Y_N, levels = c(0, 1), labels = c("Not Divorced", "Divorced"))) %>%
pivot_longer(cols = all_of(low_vars), names_to = "Variable", values_to = "Score")
# Boxplot
ggplot(plot_data, aes(x = Divorce_Y_N, y = Score)) +
geom_boxplot() +
facet_wrap(~ Variable) +
labs(x = "Divorce Status", y = "Score")
# Written by: Casper Collet
# Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
# Written by Casper Collet
# Here we test the accuracy of the model on the unseendata, this came down to 100%, because it had perfect accuracy.
library(caret)
Unseendata$Divorce_Y_N <- factor(Unseendata$Divorce_Y_N, levels = levels(seendata$Divorce_Y_N))
unseen_predictions <- predict(rf_model, Unseendata)
# Evaluate the accuracy on Unseendata
conf_matrix_unseen <- confusionMatrix(unseen_predictions, Unseendata$Divorce_Y_N)
print(conf_matrix_unseen)
# Written by: Casper Collet
# We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model)
varImpPlot(rf_model)
Randomforest_most_important_questions <- importance(rf_model) #To compare later on
# Written by: Casper Collet
# Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
print(bagging_model)
# We immediately see that in the bagging method, has one less falsely classified object in the confusion matrix (4 became 3). Next on we test the bagging method on the unseen data. This show us an accuracy of 77.14% which is less than the 100% from random forest.
unseen_predictions <- predict(bagging_model, Unseendata)
# Evaluate the accuracy on Unseendata
conf_matrix_unseenbag <- confusionMatrix(unseen_predictions, Unseendata$Divorce_Y_N)
print(conf_matrix_unseenbag)
# Here we show the most important variables from the bagging model.
importance(bagging_model)
varImpPlot(bagging_model)
Bagging_most_important_questions <- importance(bagging_model)
# written by: Casper Collet
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.
# Load required packages
library(glmnet)
# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric response
# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
lasso_best_lambda <- lasso_model$lambda.min  # Get the best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)  # Extract coefficients
# Print results
print("Lasso Model Coefficients:")
print(lasso_coefficients)
# written by: Chu Li
# Last but not least, the Ridge model.
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients
print("Ridge Model Coefficients:")
print(ridge_coefficients)
# written by: Chu Li
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance
# Load necessary libraries
library(pROC)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Prepare data matrices for Lasso and Ridge (glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorce_Y_N))  # Convert factor to numeric
# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
lasso_pred <- predict(lasso_model, newx = x_test, s = "lambda.min", type = "response")
lasso_class <- ifelse(lasso_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Ridge Model (L2 regularization)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
ridge_pred <- predict(ridge_model, newx = x_test, s = "lambda.min", type = "response")
ridge_class <- ifelse(ridge_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Lasso
lasso_results <- evaluate_model(y_test, lasso_class, lasso_pred)
print("Lasso Model Results:")
print(lasso_results)
# Evaluate Ridge
ridge_results <- evaluate_model(y_test, ridge_class, ridge_pred)
print("Ridge Model Results:")
print(ridge_results)
# Plot AUC-ROC Curves
roc_lasso <- roc(y_test, lasso_pred)
roc_ridge <- roc(y_test, ridge_pred)
plot(roc_lasso, col = "blue", main = "AUC-ROC Curves for Lasso and Ridge")
plot(roc_ridge, col = "red", add = TRUE)
legend("bottomright", legend = c("Lasso", "Ridge"), col = c("blue", "red"), lty = 1)
# written by: Chu Li
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)
# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_class, rf_pred)
print("Random Forest Model Results:")
print(rf_results)
# Evaluate Bagging
bagging_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_class, bagging_pred)
print("Bagging Model Results:")
print(bagging_results)
# Plot AUC-ROC Curves
roc_rf <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_pred)
roc_bagging <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_pred)
plot(roc_rf, col = "blue", main = "AUC-ROC Curves for Random Forest and Bagging")
plot(roc_bagging, col = "red", add = TRUE)
legend("bottomright", legend = c("Random Forest", "Bagging"), col = c("blue", "red"), lty = 1)
#Written by: Casper Collet & Chu Li
library(tidyverse)
library(readxl)
# Load dataset
data_file <- "divorce.xlsx"
if (!file.exists(data_file)) {
stop("Error: Data file 'divorce.xlsx' not found.")
}
divorce_data <- read_xlsx("divorce.xlsx")
# Check total rows
cat("Total rows in divorce_data:", nrow(divorce_data), "\n")
# Check for missing values
if (any(is.na(divorce_data))) {
warning("Missing values detected. Removing rows with NAs.")
divorce_data <- divorce_data %>%
drop_na()
cat("Rows after removing NAs:", nrow(divorce_data), "\n")
} else {
cat("No missing values detected.\n")
}
# Check for duplicates
duplicates <- divorce_data %>%
duplicated() %>%
sum()
cat("Number of duplicate rows:", duplicates, "\n")
# Add row index for splitting
divorce_data <- divorce_data %>%
mutate(.row = row_number())
# Split into 80% training and 20% testing sets
set.seed(123)
Unseendata <- divorce_data %>%
slice_sample(prop = 0.2)  # 34 rows
seendata <- divorce_data %>%
slice(-Unseendata$.row)   # Remaining 136 rows
# Remove temporary row index
Unseendata <- Unseendata %>% select(-.row)
seendata <- seendata %>% select(-.row)
# Rename columns
divorce_data <- divorce_data %>%
rename(
Two_Strangers = `2_strangers`,
idk_what_is_going_on = `idk_what's_going_on`,
I_Am_Right = `I'm_right`,
I_am_not_wrong = `I'm_not_wrong`,
I_am_not_guilty = `I'm_not_guilty`,
you_are_inadequate = `you're_inadequate`
)
Unseendata <- Unseendata %>%
rename(
Two_Strangers = `2_strangers`,
idk_what_is_going_on = `idk_what's_going_on`,
I_Am_Right = `I'm_right`,
I_am_not_wrong = `I'm_not_wrong`,
I_am_not_guilty = `I'm_not_guilty`,
you_are_inadequate = `you're_inadequate`
)
seendata <- seendata %>%
rename(
Two_Strangers = `2_strangers`,
idk_what_is_going_on = `idk_what's_going_on`,
I_Am_Right = `I'm_right`,
I_am_not_wrong = `I'm_not_wrong`,
I_am_not_guilty = `I'm_not_guilty`,
you_are_inadequate = `you're_inadequate`
)
# Convert Divorce_Y_N to factor
seendata <- seendata %>%
mutate(Divorce_Y_N = as_factor(Divorce_Y_N))
Unseendata <- Unseendata %>%
mutate(Divorce_Y_N = as_factor(Divorce_Y_N))
# Verify row counts
cat("Rows in seendata:", nrow(seendata), "\n")      # Should be 136
cat("Rows in Unseendata:", nrow(Unseendata), "\n")  # Should be 34
cat("Total rows:", nrow(seendata) + nrow(Unseendata), "\n")  # Should be 170
head(divorce_data)
str(divorce_data)
table(Unseendata$Divorce_Y_N, useNA = "always")
