divorce_data <- read_excel("divorce.xlsx")
head(divorce_data)
str(divorce_data)
#Written by: Casper Collet
#A couple of variables had backticks in them and needed to be changed to ensure we can work with the dataset.
colnames(divorce_data)[colnames(divorce_data) == "2_strangers"] <- "Two_Strangers"
colnames(divorce_data)[colnames(divorce_data) == "idk_what's_going_on"] <- "idk_what_is_going_on"
colnames(divorce_data)[colnames(divorce_data) == "I'm_right"] <- "I_Am_Right"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_wrong"] <- "I_am_not_wrong"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_guilty"] <- "I_am_not_guilty"
colnames(divorce_data)[colnames(divorce_data) ==  "you're_inadequate"] <- "you_are_inadequate"
#Our first task is: Split around 20% of the dataset of so we can keep some unseen data to test our models on after we tried them on the 80%. To keep it random, we will remove the last.
Unseendata <- divorce_data[136:170, ]
seendata <- divorce_data[1:135, ]
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
install.packages("randomForest")
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorced <- as.factor(seendata$Divorce_Y_N)
seendata <- subset(seendata, select = -Divorce_Y_N)
rf_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
install.packages("randomForest")
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
#Written by: Casper Collet
install.packages("readxl")  # Run this only once
library(readxl)
divorce_data <- read_excel("divorce.xlsx")
head(divorce_data)
str(divorce_data)
#Written by: Casper Collet
#A couple of variables had backticks in them and needed to be changed to ensure we can work with the dataset.
colnames(divorce_data)[colnames(divorce_data) == "2_strangers"] <- "Two_Strangers"
colnames(divorce_data)[colnames(divorce_data) == "idk_what's_going_on"] <- "idk_what_is_going_on"
colnames(divorce_data)[colnames(divorce_data) == "I'm_right"] <- "I_Am_Right"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_wrong"] <- "I_am_not_wrong"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_guilty"] <- "I_am_not_guilty"
colnames(divorce_data)[colnames(divorce_data) ==  "you're_inadequate"] <- "you_are_inadequate"
#Our first task is: Split around 20% of the dataset of so we can keep some unseen data to test our models on after we tried them on the 80%. To keep it random, we will remove the last.
Unseendata <- divorce_data[136:170, ]
seendata <- divorce_data[1:135, ]
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
install.packages("randomForest")
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
rf_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
install.packages("randomForest")
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
#Written by: Casper Collet
#We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model)
#Written by: Casper Collet
install.packages("readxl")  # Run this only once
library(readxl)
divorce_data <- read_excel("divorce.xlsx")
head(divorce_data)
str(divorce_data)
#Written by: Casper Collet
#A couple of variables had backticks in them and needed to be changed to ensure we can work with the dataset.
colnames(divorce_data)[colnames(divorce_data) == "2_strangers"] <- "Two_Strangers"
colnames(divorce_data)[colnames(divorce_data) == "idk_what's_going_on"] <- "idk_what_is_going_on"
colnames(divorce_data)[colnames(divorce_data) == "I'm_right"] <- "I_Am_Right"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_wrong"] <- "I_am_not_wrong"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_guilty"] <- "I_am_not_guilty"
colnames(divorce_data)[colnames(divorce_data) ==  "you're_inadequate"] <- "you_are_inadequate"
#Our first task is: Split around 20% of the dataset of so we can keep some unseen data to test our models on after we tried them on the 80%. To keep it random, we will remove the last.
Unseendata <- divorce_data[136:170, ]
seendata <- divorce_data[1:135, ]
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
install.packages("randomForest")
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
#Written by: Casper Collet
#We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model)
varImpPlot(rf_model)
Randomforest_most_important_questions <- importance(rf_model) #To compare later on
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
#Written by: Casper Collet
#We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model)
varImpPlot(rf_model)
Randomforest_most_important_questions <- importance(rf_model) #To compare later on
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(divorce_data ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
print(bagging_model)
#We immediately see that in the bagging method, has one less falsely classified object in the confusion matrix (4 became 3).
importance(bagging_model)
varImpPlot(bagging_model)
Bagging_most_important_questions <- importance(bagging_model)
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.(C.L)
# Load required packages
install.packages("glmnet")  # Run only once if not installed
library(glmnet)
# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorced))  # Convert factor to numeric response
# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.(C.L)
# Load required packages
install.packages("glmnet")  # Run only once if not installed
library(glmnet)
# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorce))  # Convert factor to numeric response
# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.(C.L)
# Load required packages
install.packages("glmnet")  # Run only once if not installed
library(glmnet)
# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric response
# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
lasso_best_lambda <- lasso_model$lambda.min  # Get the best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)  # Extract coefficients
# Print results
print("Lasso Model Coefficients:")
print(lasso_coefficients)
#Last but not least, the Ridge model.(C.L)
install.packages("glmnet", dependencies = TRUE)
library(glmnet)
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients
print("Ridge Model Coefficients:")
print(ridge_coefficients)
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)
# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation
library(glmnet)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorced <- as.factor(Unseendata$Divorced)
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)
# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation
library(glmnet)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorced)
install.packages("caret")
install.packages("glmnet")
install.packages("pROC")
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)
# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation
library(glmnet)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Prepare data matrices for Lasso and Ridge (glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorced))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorced))  # Convert factor to numeric
# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
length(y_train)
str(y_train)
table(y_train, useNA = "always")
colnames(seendata)  # 看看是否有 "Divorced_Y_N"
# 查看所有包含 "Divorc" 相关的列
colnames(seendata)[grepl("(?i)divorc", colnames(seendata))]
# 只修改特定的错误列
colnames(seendata)[colnames(seendata) == "Divorced"] <- "Divorce_Y_N"
colnames(seendata)[colnames(seendata) == "Divorced_YN"] <- "Divorce_Y_N"
colnames(seendata)[colnames(seendata) == "Divorced_Y_NN"] <- "Divorce_Y_N"
table(seendata$Divorce_Y_N, useNA = "always")
# 查看列名
colnames(Unseendata)
# 查看前几行数据
head(Unseendata)
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)
# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation
library(glmnet)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Prepare data matrices for Lasso and Ridge (glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorce_Y_N))  # Convert factor to numeric
# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
lasso_pred <- predict(lasso_model, newx = x_test, s = "lambda.min", type = "response")
lasso_class <- ifelse(lasso_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Ridge Model (L2 regularization)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
ridge_pred <- predict(ridge_model, newx = x_test, s = "lambda.min", type = "response")
ridge_class <- ifelse(ridge_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Lasso
lasso_results <- evaluate_model(y_test, lasso_class, lasso_pred)
table(y_test, useNA = "always")
table(Unseendata$Divorce_Y_N, useNA = "always")
#Written by: Casper Collet
install.packages("readxl")  # Run this only once
library(readxl)
divorce_data <- read_excel("divorce.xlsx")
head(divorce_data)
str(divorce_data)
#Written by: Casper Collet
#A couple of variables had backticks in them and needed to be changed to ensure we can work with the dataset.
colnames(divorce_data)[colnames(divorce_data) == "2_strangers"] <- "Two_Strangers"
colnames(divorce_data)[colnames(divorce_data) == "idk_what's_going_on"] <- "idk_what_is_going_on"
colnames(divorce_data)[colnames(divorce_data) == "I'm_right"] <- "I_Am_Right"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_wrong"] <- "I_am_not_wrong"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_guilty"] <- "I_am_not_guilty"
colnames(divorce_data)[colnames(divorce_data) ==  "you're_inadequate"] <- "you_are_inadequate"
#Our first task is: Split around 20% of the dataset of so we can keep some unseen data to test our models on after we tried them on the 80%. To keep it random, we will remove the last.
set.seed(123)
sample_index <- sample(1:nrow(divorce_data), size = round(0.2 * nrow(divorce_data)))
Unseendata <- divorce_data[sample_index, ]
seendata <- divorce_data[-sample_index, ]
table(Unseendata$Divorce_Y_N, useNA = "always")
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.
install.packages("randomForest")
library(randomForest)
#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.
seendata$Divorce_Y_N <- as.factor(seendata$Divorce_Y_N)
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
#Written by: Casper Collet
#We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model)
varImpPlot(rf_model)
Randomforest_most_important_questions <- importance(rf_model) #To compare later on
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights.
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
print(bagging_model)
#We immediately see that in the bagging method, has one less falsely classified object in the confusion matrix (4 became 3).
importance(bagging_model)
varImpPlot(bagging_model)
Bagging_most_important_questions <- importance(bagging_model)
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.(C.L)
# Load required packages
install.packages("glmnet")  # Run only once if not installed
library(glmnet)
# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric response
# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
lasso_best_lambda <- lasso_model$lambda.min  # Get the best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)  # Extract coefficients
# Print results
print("Lasso Model Coefficients:")
print(lasso_coefficients)
#Last but not least, the Ridge model.(C.L)
install.packages("glmnet", dependencies = TRUE)
library(glmnet)
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients
print("Ridge Model Coefficients:")
print(ridge_coefficients)
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)
# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation
library(glmnet)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Prepare data matrices for Lasso and Ridge (glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorce_Y_N))  # Convert factor to numeric
# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
#Last but not least, the Ridge model.(C.L)
install.packages("glmnet", dependencies = TRUE)
library(glmnet)
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients
print("Ridge Model Coefficients:")
print(ridge_coefficients)
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)
# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation
library(glmnet)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Prepare data matrices for Lasso and Ridge (glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorce_Y_N))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorce_Y_N))  # Convert factor to numeric
# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
lasso_pred <- predict(lasso_model, newx = x_test, s = "lambda.min", type = "response")
lasso_class <- ifelse(lasso_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Ridge Model (L2 regularization)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
ridge_pred <- predict(ridge_model, newx = x_test, s = "lambda.min", type = "response")
ridge_class <- ifelse(ridge_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Lasso
lasso_results <- evaluate_model(y_test, lasso_class, lasso_pred)
print("Lasso Model Results:")
print(lasso_results)
# Evaluate Ridge
ridge_results <- evaluate_model(y_test, ridge_class, ridge_pred)
print("Ridge Model Results:")
print(ridge_results)
# Plot AUC-ROC Curves
roc_lasso <- roc(y_test, lasso_pred)
roc_ridge <- roc(y_test, ridge_pred)
plot(roc_lasso, col = "blue", main = "AUC-ROC Curves for Lasso and Ridge")
plot(roc_ridge, col = "red", add = TRUE)
legend("bottomright", legend = c("Lasso", "Ridge"), col = c("blue", "red"), lty = 1)
# Load necessary libraries
install.packages("randomForest")  # Random Forest and Bagging
install.packages("pROC")          # AUC-ROC calculation
install.packages("caret")          # Accuracy calculation
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorced <- as.factor(Unseendata$Divorce_Y_N)
Unseendata <- subset(Unseendata, select = -Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
# Load necessary libraries
install.packages("randomForest")  # Random Forest and Bagging
install.packages("pROC")          # AUC-ROC calculation
install.packages("caret")          # Accuracy calculation
library(randomForest)
library(pROC)
library(caret)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorced <- as.factor(Unseendata$Divorce_Y_N)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
colnames(Unseendata)
colnames(seendata)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
colnames(Unseendata)[colnames(Unseendata) == "Divorced"] <- "Divorce_Y_N"
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)
# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorced)), rf_class, rf_pred)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
colnames(Unseendata)[colnames(Unseendata) == "Divorced"] <- "Divorce_Y_N"
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)
# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_class, rf_pred)
print("Random Forest Model Results:")
print(rf_results)
# Evaluate Bagging
bagging_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_class, bagging_pred)
print("Bagging Model Results:")
print(bagging_results)
# Plot AUC-ROC Curves
roc_rf <- roc(as.numeric(as.character(Unseendata$Divorced)), rf_pred)
# Load necessary libraries
library(randomForest)
library(pROC)
library(caret)
# Convert Divorce label to factor for classification
colnames(Unseendata)[colnames(Unseendata) == "Divorced"] <- "Divorce_Y_N"
Unseendata$Divorce_Y_N <- as.factor(Unseendata$Divorce_Y_N)
# Train Random Forest Model
rf_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
rf_pred <- predict(rf_model, newdata = Unseendata, type = "prob")[,2]  # Extract probability scores
rf_class <- ifelse(rf_pred > 0.5, 1, 0)  # Convert probabilities to binary labels
# Train Bagging Model
bagging_model <- randomForest(Divorce_Y_N ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)
bagging_pred <- predict(bagging_model, newdata = Unseendata, type = "prob")[,2]
bagging_class <- ifelse(bagging_pred > 0.5, 1, 0)
# Function to compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) {
accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
return(list(accuracy = accuracy, AUC = auc, MSE = mse))
}
# Evaluate Random Forest
rf_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_class, rf_pred)
print("Random Forest Model Results:")
print(rf_results)
# Evaluate Bagging
bagging_results <- evaluate_model(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_class, bagging_pred)
print("Bagging Model Results:")
print(bagging_results)
# Plot AUC-ROC Curves
roc_rf <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), rf_pred)
roc_bagging <- roc(as.numeric(as.character(Unseendata$Divorce_Y_N)), bagging_pred)
plot(roc_rf, col = "blue", main = "AUC-ROC Curves for Random Forest and Bagging")
plot(roc_bagging, col = "red", add = TRUE)
legend("bottomright", legend = c("Random Forest", "Bagging"), col = c("blue", "red"), lty = 1)
