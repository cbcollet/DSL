---
title: "DSL FINAL CODE"
author: "Casper Collet and Chu Li"
date: "2025-03-29"
output: html_document
---

```{r}
#Written by: Casper Collet
install.packages("readxl")  # Run this only once
library(readxl)

divorce_data <- read_excel("divorce.xlsx")
head(divorce_data)
str(divorce_data)

#Written by: Casper Collet

#A couple of variables had backticks in them and needed to be changed to ensure we can work with the dataset.

colnames(divorce_data)[colnames(divorce_data) == "2_strangers"] <- "Two_Strangers"
colnames(divorce_data)[colnames(divorce_data) == "idk_what's_going_on"] <- "idk_what_is_going_on"
colnames(divorce_data)[colnames(divorce_data) == "I'm_right"] <- "I_Am_Right"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_wrong"] <- "I_am_not_wrong"
colnames(divorce_data)[colnames(divorce_data) ==  "I'm_not_guilty"] <- "I_am_not_guilty"
colnames(divorce_data)[colnames(divorce_data) ==  "you're_inadequate"] <- "you_are_inadequate"

#Our first task is: Split around 20% of the dataset of so we can keep some unseen data to test our models on after we tried them on the 80%. To keep it random, we will remove the last.

Unseendata <- divorce_data[136:170, ]
seendata <- divorce_data[1:135, ]
```

```{r}
#Written by: Casper Collet
#Now that we have data to work with (the remaining 80%) we can try different models to test predictability power. We start with random forest. We choos deliberatly to not split the seendata again in traindata and testdata because we have a relatively small dataset. Otherwise the machine learning might be less accurate and we don't want that.

install.packages("randomForest")
library(randomForest)

#To ensure the last variables not interfering with the tests, we make it a binary valuable with either 1 or 0 and remove the old variable. This is just to be sure.

seendata$Divorced <- as.factor(seendata$Divorce_Y_N)
seendata <- subset(seendata, select = -Divorce_Y_N)


rf_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = 3, importance = TRUE)
print(rf_model)
```

```{r}
#Written by: Casper Collet
#We can use this random forest also to check for the most important questions for predicting divorce, after we use the different methods we can see if they also validate these.
importance(rf_model) 
varImpPlot(rf_model) 
Randomforest_most_important_questions <- importance(rf_model) #To compare later on

```

```{r}
#Written by: Casper Collet
#Secondly, we will try the method Bagging (Bootstrap Aggregating). This is quite similar to RandomForest, but might give us some new insights. 

bagging_model <- randomForest(Divorced ~ ., data = seendata, ntree = 500, mtry = ncol(seendata) - 1, importance = TRUE)

print(bagging_model)

#We immediately see that in the bagging method, has one less falsely classified object in the confusion matrix (4 became 3).

importance(bagging_model)
varImpPlot(bagging_model)
Bagging_most_important_questions <- importance(bagging_model)
```

```{r}
#Next up is the Lasso model (Least Absolute Shrinkage and Selection Operator). We have no categorical objects and already made sure before that the Divorce column is binary.(C.L)

# Load required packages
install.packages("glmnet")  # Run only once if not installed
library(glmnet)

# Prepare the data for Lasso and Ridge models
# Convert the dataset to a matrix format suitable for glmnet
x <- as.matrix(seendata[, -ncol(seendata)])  # All predictor variables
y <- as.numeric(as.character(seendata$Divorced))  # Convert factor to numeric response

# Fit Lasso model
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # L1 regularization
lasso_best_lambda <- lasso_model$lambda.min  # Get the best lambda
lasso_coefficients <- coef(lasso_model, s = lasso_best_lambda)  # Extract coefficients
# Print results
print("Lasso Model Coefficients:")
print(lasso_coefficients)
```

```{r}
#Last but not least, the Ridge model.(C.L)
ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # L2 regularization
ridge_best_lambda <- ridge_model$lambda.min  # Get the best lambda
ridge_coefficients <- coef(ridge_model, s = ridge_best_lambda)  # Extract coefficients

print("Ridge Model Coefficients:")
print(ridge_coefficients)
```

```{r}
# Evaluate 2 models (Lasso, Ridge) using Unseendata. Then calculate Accuracy, AUC-ROC curves, and Mean Square Error (MSE) for each model. Finally Visualize AUC-ROC curves to compare model performance (C.L)

# Load necessary libraries
install.packages("glmnet") # Lasso and Ridge
install.packages("pROC")   # AUC-ROC calculation
install.packages("caret")   # Accuracy calculation

library(glmnet)
library(pROC)
library(caret)

# Convert Divorce label to factor for classification
Unseendata$Divorced <- as.factor(Unseendata$`Divorce_Y_N`)
Unseendata$Divorced <- as.factor(Unseendata$Divorce_Y_N)
Unseendata <- subset(Unseendata, select = -Divorce_Y_N)

# Prepare data matrices for Lasso and Ridge (since glmnet requires matrix input)
x_train <- as.matrix(seendata[, -ncol(seendata)])  # Features from seen data
y_train <- as.numeric(as.character(seendata$Divorced))  # Convert factor to numeric
x_test <- as.matrix(Unseendata[, -ncol(Unseendata)])  # Features from unseen data
y_test <- as.numeric(as.character(Unseendata$Divorced))  # Convert factor to numeric

# Train Lasso Model (L1 regularization)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
lasso_pred <- predict(lasso_model, newx = x_test, s = "lambda.min", type = "response")
lasso_class <- ifelse(lasso_pred > 0.5, 1, 0)  # Convert probabilities to binary labels

# Train Ridge Model (L2 regularization)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")
ridge_pred <- predict(ridge_model, newx = x_test, s = "lambda.min", type = "response")
ridge_class <- ifelse(ridge_pred > 0.5, 1, 0)  # Convert probabilities to binary labels

# Compute evaluation metrics
evaluate_model <- function(true_labels, predicted_labels, predicted_probs) 
  accuracy <- sum(true_labels == predicted_labels) / length(true_labels)  # Compute accuracy
  auc <- auc(roc(true_labels, predicted_probs))  # Compute AUC-ROC
  mse <- mean((true_labels - predicted_probs)^2)  # Compute Mean Squared Error
  return(list(accuracy = accuracy, AUC = auc, MSE = mse))
  
  # Evaluate Lasso
lasso_results <- evaluate_model(y_test, lasso_class, lasso_pred)
print("Lasso Model Results:")
print(lasso_results)

# Evaluate Ridge
ridge_results <- evaluate_model(y_test, ridge_class, ridge_pred)
print("Ridge Model Results:")
print(ridge_results)

# Plot AUC-ROC Curves
roc_lasso <- roc(y_test, lasso_pred)
roc_ridge <- roc(y_test, ridge_pred)

plot(roc_lasso, col = "blue", main = "AUC-ROC Curves for Lasso and Ridge")
plot(roc_ridge, col = "red", add = TRUE)
legend("bottomright", legend = c("Lasso", "Ridge"), col = c("blue", "red"), lty = 1)

```

```{r}
colnames(Unseendata)

```

```{r}
remove.packages("xfun")   
install.packages("xfun")  
library(xfun)
packageVersion("xfun")  

```

```{r}
library(xfun)
packageVersion("xfun")

```



```